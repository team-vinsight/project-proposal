\section{Methodology}

\subsection{Proposed Solution}
For this project, we propose the development and implementation of a hybrid, optimization-based Visual-Inertial Navigation System (VINS). This system will be built upon a robust, state-of-the-art geometric SLAM framework, such as ORB-SLAM3, and will be augmented with a real-time, lightweight deep learning module for dynamic object segmentation. This approach is designed to synergistically combine the geometric precision of classical methods with the semantic scene understanding of modern AI.

\subsubsection{Why It Is Chosen}
This hybrid architecture was selected as it offers the most pragmatic and robust pathway to achieving high-performance autonomous navigation on a UAV.

\begin{itemize}
    \item \textbf{Synergy of Paradigms:} It leverages the decades of research that have gone into making geometric SLAM systems like ORB-SLAM3 incredibly accurate and efficient. Instead of replacing this mature technology, we augment it, using deep learning to solve a problem that is intractable for pure geometry: semantic understanding. This ``best of both worlds'' approach has been shown to be highly effective in recent research~\cite{refnew13}.
    \item \textbf{Targeted Problem Solving:} The proposed solution directly addresses the most common failure mode for VINS in real-world environments: dynamic objects. By explicitly detecting and rejecting data from moving entities, the system's core geometric estimator is protected from corruption, dramatically increasing its reliability.
    \item \textbf{Feasibility for Embedded Deployment:} A key design constraint for any UAV system is real-time performance on SWaP-constrained hardware. Full deep-learning systems are often too demanding. However, modern lightweight object detection models like YOLO are highly optimized and have been demonstrated to run at high frame rates on embedded AI platforms like the NVIDIA Jetson series, which are commonly used on research UAVs~\cite{refnew14}. This makes our proposed hybrid architecture computationally feasible.
\end{itemize}

\subsubsection{Advantages}
The expected benefits of this solution are threefold:

\begin{itemize}
    \item \textbf{Superior Localization Accuracy:} By systematically identifying and excluding unstable features on moving objects, the bundle adjustment algorithm will operate on a cleaner, more consistent set of data. This will lead to a significant reduction in pose drift and a more accurate trajectory estimate, particularly over long-duration missions in dynamic environments.
    \item \textbf{Unprecedented System Robustness:} The system will be resilient to dynamic disturbances that would cause traditional VINS to lose tracking or fail completely. This robustness is essential for safe and reliable autonomous operation in unpredictable, human-centric environments.
    \item \textbf{Foundation for Higher-Level Autonomy:} The semantic information generated by the object detection module is not just useful for filtering; it creates a semantic map of the environment. This map, which annotates objects with their class labels (e.g., ``person'', ``vehicle''), is an invaluable input for advanced autonomy tasks such as risk-aware motion planning, safe human-robot interaction, and context-aware decision making~\cite{refnew15}.
\end{itemize}

\subsubsection{Disadvantages}
Despite the clear advantages, several challenges and potential drawbacks must be acknowledged and addressed:

\begin{itemize}
    \item \textbf{Increased System Latency and Complexity:} Integrating a deep learning pipeline into a real-time SLAM system adds complexity. The inference time of the neural network introduces a latency between when an image is captured and when its semantic content is known. This requires careful timestamping and buffer management to ensure that the correct features are filtered in the tracking thread without disrupting the real-time operation of the system~\cite{refnew16}.
    \item \textbf{Brittleness of the Perception Module:} The system's robustness to dynamics is fundamentally bounded by the performance of the object detector. It will be unable to handle moving objects for which it has not been trained. Furthermore, false positives (misclassifying a static object as dynamic) could lead to the unnecessary removal of stable map points, which could weaken the system's ability to localize, especially in environments that are already sparse in features.
    \item \textbf{Computational Resource Management:} While feasible, running both a full SLAM system and a CNN simultaneously on an embedded computer requires careful resource management. CPU and GPU usage must be monitored and potentially balanced to prevent thermal throttling and ensure that no single process starves the others of necessary computational resources, which could compromise the stability of the entire system.
\end{itemize}

\subsection{Datasets \& Benchmark-First Validation Plan}
To de-risk real-world flights and enable fair comparison with prior work, we adopt a benchmark-first strategy before hardware-in-the-loop testing. We will:
\begin{enumerate}
    \item Bring-up and tune on \textbf{EuRoC MAV} (stereo+IMU, indoor lab/factory sequences),
    \item Stress-test on \textbf{TUM-VI} (stereo+IMU, HDR/low-light, wide-FOV),
    \item Validate outdoor generalization on \textbf{KITTI} (stereo images; KITTI Raw for OXTS GPS/IMU when needed).
\end{enumerate}

Optional stress datasets include \textbf{UZH-FPV} (aggressive drone trajectories) and \textbf{Hilti-Oxford} (millimetre-level ground truth) once the pipeline is stable. We evaluate per dataset using identical metrics and configurations (see Evaluation Methodology), and only after passing pre-specified thresholds (see Success Criteria) do we proceed to embedded deployment and flight tests.

\subsection{System Architecture}
The proposed system follows a modular pipeline:
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, trim=1cm 1cm 1cm 1cm, clip]{images/architecture.jpg}
    \caption{System Architecture}
    \label{fig:System_architecture}
\end{figure}

\begin{itemize}
    \item \textbf{Sensors:} Stereo/mono camera(s) + IMU with hardware time-sync.
    \item \textbf{Pre-processing:} Camera undistortion/rectification; IMU de-biasing; strict timestamp alignment.
    \item \textbf{Front-end:} Feature detection/description; dynamic-object masking via a lightweight detector with confidence gating and temporal smoothing.
    \item \textbf{VIO Tracking:} Visual--inertial fusion with robust outlier rejection; re-initialization logic for tracking loss.
    \item \textbf{Back-end:} Sliding-window bundle adjustment optimizing poses, landmarks, and IMU biases; optional loop closure in mapped zones.
    \item \textbf{State Output:} Pose, velocity, and covariance streams at 20--30 Hz.
    \item \textbf{UAV Controller:} PX4/ArduPilot via MAVLink or ROS~2; offboard control for GPS-denied waypoint flight.
\end{itemize}

\textbf{Real-time targets:} 30 Hz input, $\leq$ 40 ms end-to-end latency, $\leq$ 80\% GPU utilization on Jetson-class hardware.

\subsection{Hardware/Software Configuration}
\begin{itemize}
    \item \textbf{Compute:} NVIDIA Jetson Orin NX 8 GB (Ubuntu 22.04; CUDA/cuDNN pinned).
    \item \textbf{Flight stack:} PX4 $\geq$ v1.14; MAVLink; MAVSDK/MAVROS (ROS 2 Humble).
    \item \textbf{Sensors:} Stereo global-shutter camera @ 640Ã—480, 30 Hz; IMU with $\leq$ 1 ms time-sync; AprilTag board for calibration.
    \item \textbf{Libraries:} OpenCV, Eigen, Ceres/G2O, Sophus; ORB-SLAM3 (VIO mode) or equivalent; lightweight detector (YOLO-Tiny/Nano) with INT8 quantization when needed.
    \item \textbf{Build \& CI:} Version-pinned environments, reproducible scripts, and continuous evaluation against EuRoC/TUM-VI/KITTI.
\end{itemize}

\subsection{Evaluation Methodology}
We evaluate across three settings:
\begin{enumerate}
    \item Indoor low-texture corridors,
    \item Outdoor dynamic scenes (people/vehicles),
    \item Stress tests (low-light, motion blur).
\end{enumerate}

For each sequence we report:
\begin{itemize}
    \item \textbf{Accuracy:} Absolute Trajectory Error (ATE) and Relative Pose Error (RPE).
    \item \textbf{Robustness:} Tracking uptime (\% of frames with valid pose); time-to-recover after induced loss.
    \item \textbf{Throughput:} End-to-end FPS and median per-frame latency on the target embedded device.
    \item \textbf{Ablations:} With/without dynamic-object masking; detector thresholds; mono vs stereo; photometric calibration on/off (for TUM-VI).
\end{itemize}

Each scenario is repeated $\geq$ 5 runs; we report mean $\pm$ std and best/worst cases. Evaluation is performed with the \texttt{evo} toolkit (consistent configs and seeds).

\begin{table}[H]
\centering
\small
\caption{Evaluation Scenarios and Pass Criteria}
\begin{tabular}{|p{3.0cm}|p{3.0cm}|p{3.0cm}|p{5.0cm}|}
\hline
\textbf{Scenario} & \textbf{Conditions} & \textbf{Metrics} & \textbf{Pass Criteria} \\

\hline
Indoor corridor & Low-texture, good light & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE
\item RPE
\item FPS
\end{itemize} & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE $\leq$ 0.25 m
\item RPE $\leq$ 2\%
\item $\geq$ 20 FPS
\end{itemize} \\

\hline
Outdoor dynamic & Pedestrians/vehicles & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE
\item Uptime
\item Recovery
\end{itemize} & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE $\leq$ 0.50 m
\item Uptime $\geq$ 95\%
\item Recovery $\leq$ 2 s
\end{itemize} \\

\hline
Low-light & $< 100$ lux & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE
\item Uptime
\end{itemize} & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item ATE $\leq$ 0.60 m
\item Uptime $\geq$ 90\%
\end{itemize} \\

\hline
Motion blur & Rapid yaw/pitch & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item RPE
\item Recovery
\end{itemize} & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item RPE $\leq$ 3\%
\item Recovery $\leq$ 2 s
\end{itemize} \\

\hline
Ablation: no masks & Same as outdoor & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item $\Delta$ATE,
\item $\Delta$Uptime
\end{itemize} & 
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt, parsep=0pt, partopsep=0pt]
\item Degradation $\leq$ 30\% vs. masked 
\end{itemize} \\

\hline
\end{tabular}
\label{tab:evaluation_criteria}
\end{table}

\subsection{Success Criteria (Definition of Done)}
\begin{itemize}
    \item \textbf{Accuracy:} ATE $\leq$ 0.25 m (indoor) and $\leq$ 0.50 m (outdoor); RPE $\leq$ 2\% over 100 m.
    \item \textbf{Robustness:} $\geq$ 95\% tracking uptime in dynamic scenes; $\leq$ 2 s recovery after induced loss.
    \item \textbf{Real-time:} $\geq$ 20 FPS full pipeline on Jetson Orin NX 8 GB with $\leq$ 80\% GPU utilization.
    \item \textbf{Demo:} GPS-denied waypoint flight with live pose feedback and log replay.
    \item \textbf{Reproducibility:} Version-pinned environments, scripts, and public configs.
\end{itemize}


